{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting geomloss\n",
      "  Downloading geomloss-0.2.6.tar.gz (26 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy (from geomloss)\n",
      "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "Collecting torch (from geomloss)\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch->geomloss)\n",
      "  Using cached filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Collecting typing-extensions (from torch->geomloss)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting sympy (from torch->geomloss)\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx (from torch->geomloss)\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Collecting jinja2 (from torch->geomloss)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch->geomloss)\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch->geomloss)\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch->geomloss)\n",
      "  Using cached nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch->geomloss)\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch->geomloss)\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch->geomloss)\n",
      "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "Collecting nvidia-curand-cu11==10.2.10.91 (from torch->geomloss)\n",
      "  Using cached nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch->geomloss)\n",
      "  Using cached nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch->geomloss)\n",
      "  Using cached nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "Collecting nvidia-nccl-cu11==2.14.3 (from torch->geomloss)\n",
      "  Using cached nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "Collecting nvidia-nvtx-cu11==11.7.91 (from torch->geomloss)\n",
      "  Using cached nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "Collecting triton==2.0.0 (from torch->geomloss)\n",
      "  Using cached triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch->geomloss) (59.6.0)\n",
      "Collecting wheel (from nvidia-cublas-cu11==11.10.3.66->torch->geomloss)\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting cmake (from triton==2.0.0->torch->geomloss)\n",
      "  Using cached cmake-3.26.3-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "Collecting lit (from triton==2.0.0->torch->geomloss)\n",
      "  Downloading lit-16.0.3.tar.gz (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2->torch->geomloss)\n",
      "  Using cached MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch->geomloss)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Building wheels for collected packages: geomloss, lit\n",
      "  Building wheel for geomloss (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for geomloss: filename=geomloss-0.2.6-py3-none-any.whl size=32245 sha256=61ffe96ab09d4b78a5620b993f1d6be26d513176cf7c069cee52077c3a19b8c0\n",
      "  Stored in directory: /home/venkat/.cache/pip/wheels/0d/c9/80/4387eb03aa215ae557869d6fe8be498fd3d3cf297db2357b67\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.3-py3-none-any.whl size=88173 sha256=8dd1af58b0b6d04af7dcade6dbf922f8775d9bce1a74715e143fd6e43b5d9278\n",
      "  Stored in directory: /home/venkat/.cache/pip/wheels/d6/81/1c/a49ba782377339294cc45c9899927b61a92e58d6ad3ac942f7\n",
      "Successfully built geomloss lit\n",
      "Installing collected packages: mpmath, lit, cmake, wheel, typing-extensions, sympy, nvidia-nccl-cu11, nvidia-cufft-cu11, nvidia-cuda-nvrtc-cu11, numpy, networkx, MarkupSafe, filelock, nvidia-nvtx-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, jinja2, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, geomloss\n",
      "Successfully installed MarkupSafe-2.1.2 cmake-3.26.3 filelock-3.12.0 geomloss-0.2.6 jinja2-3.1.2 lit-16.0.3 mpmath-1.3.0 networkx-3.1 numpy-1.24.3 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 sympy-1.12 torch-2.0.1 triton-2.0.0 typing-extensions-4.5.0 wheel-0.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip install geomloss\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from geomloss import SamplesLoss\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "unlabeled, target = torch.utils.data.random_split(mnist_trainset, [59000, 1000])\n",
    "unlabeled_loader = DataLoader(unlabeled, batch_size=256, shuffle=True)\n",
    "target_loader = DataLoader(target, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loss function using GeomLoss\n",
    "sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Sinkhorn loss: 91.40467071533203\n",
      "Sinkhorn loss: 92.88128662109375\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "num_batches = 0\n",
    "\n",
    "for (unlabeled_images, _), (target_images, _) in zip(unlabeled_loader, target_loader):\n",
    "    loss = sinkhorn_loss(unlabeled_images.view(unlabeled_images.shape[0], -1), \n",
    "                         target_images.view(target_images.shape[0], -1))\n",
    "    total_loss += loss.item()\n",
    "    num_batches += 1\n",
    "\n",
    "average_loss = total_loss / num_batches\n",
    "\n",
    "print(\"Average Sinkhorn loss:\", average_loss)\n",
    "\n",
    "#loss over entire dataset\n",
    "(unlabeled_images, _), (target_images, _) = next(zip(unlabeled_loader, target_loader))\n",
    "\n",
    "loss = sinkhorn_loss(unlabeled_images.view(unlabeled_images.shape[0], -1), \n",
    "                     target_images.view(target_images.shape[0], -1))\n",
    "\n",
    "print(\"Sinkhorn loss:\", loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Sinkhorn loss: 4501.162109375\n",
      "Epoch 2, Average Sinkhorn loss: 126.59468078613281\n",
      "Epoch 3, Average Sinkhorn loss: 117.74646759033203\n",
      "Epoch 4, Average Sinkhorn loss: 111.1210708618164\n",
      "Epoch 5, Average Sinkhorn loss: 144.47360229492188\n",
      "Epoch 6, Average Sinkhorn loss: 125.82789611816406\n",
      "Epoch 7, Average Sinkhorn loss: 119.0489273071289\n",
      "Epoch 8, Average Sinkhorn loss: 121.43645477294922\n",
      "Epoch 9, Average Sinkhorn loss: 118.54010009765625\n",
      "Epoch 10, Average Sinkhorn loss: 137.48699951171875\n",
      "Epoch 11, Average Sinkhorn loss: 115.39415740966797\n",
      "Epoch 12, Average Sinkhorn loss: 125.41036987304688\n",
      "Epoch 13, Average Sinkhorn loss: 125.08084106445312\n",
      "Epoch 14, Average Sinkhorn loss: 125.04264831542969\n",
      "Epoch 15, Average Sinkhorn loss: 131.8579559326172\n",
      "Epoch 16, Average Sinkhorn loss: 117.2967300415039\n",
      "Epoch 17, Average Sinkhorn loss: 116.53772735595703\n",
      "Epoch 18, Average Sinkhorn loss: 115.61309814453125\n",
      "Epoch 19, Average Sinkhorn loss: 119.0008316040039\n",
      "Epoch 20, Average Sinkhorn loss: 120.24353790283203\n",
      "Epoch 21, Average Sinkhorn loss: 123.98866271972656\n",
      "Epoch 22, Average Sinkhorn loss: 122.52180480957031\n",
      "Epoch 23, Average Sinkhorn loss: 118.10406494140625\n",
      "Epoch 24, Average Sinkhorn loss: 135.57464599609375\n",
      "Epoch 25, Average Sinkhorn loss: 113.01561737060547\n",
      "Epoch 26, Average Sinkhorn loss: 121.69813537597656\n",
      "Epoch 27, Average Sinkhorn loss: 132.48089599609375\n",
      "Epoch 28, Average Sinkhorn loss: 112.7239761352539\n",
      "Epoch 29, Average Sinkhorn loss: 123.2983627319336\n",
      "Epoch 30, Average Sinkhorn loss: 128.2449951171875\n",
      "Epoch 31, Average Sinkhorn loss: 120.59646606445312\n",
      "Epoch 32, Average Sinkhorn loss: 121.07086944580078\n",
      "Epoch 33, Average Sinkhorn loss: 122.44256591796875\n",
      "Epoch 34, Average Sinkhorn loss: 127.87181091308594\n",
      "Epoch 35, Average Sinkhorn loss: 120.48296356201172\n",
      "Epoch 36, Average Sinkhorn loss: 121.92851257324219\n",
      "Epoch 37, Average Sinkhorn loss: 121.28524780273438\n",
      "Epoch 38, Average Sinkhorn loss: 114.9070053100586\n",
      "Epoch 39, Average Sinkhorn loss: 129.0340576171875\n",
      "Epoch 40, Average Sinkhorn loss: 120.39717864990234\n",
      "Epoch 41, Average Sinkhorn loss: 128.75796508789062\n",
      "Epoch 42, Average Sinkhorn loss: 113.93014526367188\n",
      "Epoch 43, Average Sinkhorn loss: 113.92211151123047\n",
      "Epoch 44, Average Sinkhorn loss: 125.23455047607422\n",
      "Epoch 45, Average Sinkhorn loss: 122.40028381347656\n",
      "Epoch 46, Average Sinkhorn loss: 120.33399963378906\n",
      "Epoch 47, Average Sinkhorn loss: 132.93637084960938\n",
      "Epoch 48, Average Sinkhorn loss: 121.99958801269531\n",
      "Epoch 49, Average Sinkhorn loss: 133.84300231933594\n",
      "Epoch 50, Average Sinkhorn loss: 116.52108764648438\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from geomloss import SamplesLoss\n",
    "from torch import optim\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "subset = torch.utils.data.Subset(mnist_trainset, range(990))\n",
    "unlabeled, target = torch.utils.data.random_split(subset, [900, 90])\n",
    "\n",
    "#unlabeled, target = torch.utils.data.random_split(mnist_trainset, [59744, 256])\n",
    "unlabeled_loader = DataLoader(unlabeled, batch_size=90, shuffle=True)\n",
    "target_loader = DataLoader(target, batch_size=90, shuffle=True)\n",
    "\n",
    "# Create a loss function using GeomLoss\n",
    "sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=0.05)\n",
    "\n",
    "# Initialize weights for the unlabeled_images\n",
    "weights_unlabeled = torch.full((len(unlabeled), 1), 1.0 / len(unlabeled), requires_grad=True)\n",
    "weights_target = torch.full((len(target), 1), 1.0 / len(target), requires_grad=False)\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD([weights_unlabeled], lr=0.005)\n",
    "\n",
    "# Define a function to project weights to a simplex\n",
    "def project_to_simplex(weights):\n",
    "    return torch.clamp(weights, min=0) / torch.sum(weights)\n",
    "\n",
    "def project_simplex(v):\n",
    "        \"\"\"\n",
    "        v: PyTorch Tensor to be projected to a simplex\n",
    "\n",
    "        Returns:\n",
    "        w: PyTorch Tensor simplex projection of v\n",
    "        \"\"\"\n",
    "        z = 1\n",
    "        orig_shape = v.shape\n",
    "        v = v.view(1, -1)\n",
    "        shape = v.shape\n",
    "        with torch.no_grad():\n",
    "            mu = torch.sort(v, dim=1)[0]\n",
    "            mu = torch.flip(mu, dims=(1,))\n",
    "            cum_sum = torch.cumsum(mu, dim=1)\n",
    "            j = torch.unsqueeze(torch.arange(1, shape[1] + 1, dtype=mu.dtype, device=mu.device), 0)\n",
    "            rho = torch.sum(mu * j - cum_sum + z > 0.0, dim=1, keepdim=True) - 1.\n",
    "            rho = rho.to(int)\n",
    "            max_nn = cum_sum[torch.arange(shape[0]), rho[:, 0]]\n",
    "            theta = (torch.unsqueeze(max_nn, -1) - z) / (rho.type(max_nn.dtype) + 1)\n",
    "            w = torch.clamp(v - theta, min=0.0).view(orig_shape)\n",
    "            return w\n",
    "\n",
    "\n",
    "# Loop over the datasets 10 times\n",
    "for epoch in range(50):\n",
    "    losses = []\n",
    "    weights_unlabeled.grad = None  # Reset gradients at the beginning of each epoch\n",
    "\n",
    "    for batch_idx, ((unlabeled_images, _), (target_images, _)) in enumerate(zip(unlabeled_loader, target_loader)):\n",
    "\n",
    "        # Select the weights for the current batch\n",
    "        weights_batch = weights_unlabeled[batch_idx * unlabeled_loader.batch_size : (batch_idx + 1) * unlabeled_loader.batch_size]\n",
    "\n",
    "        # Compute Sinkhorn loss\n",
    "        loss = sinkhorn_loss(weights_batch,unlabeled_images.view(unlabeled_images.shape[0], -1), weights_target,\n",
    "                             target_images.view(target_images.shape[0], -1),\n",
    "                             )\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients for the loss\n",
    "        loss.backward()  # Gradients are accumulated over mini-batches\n",
    "\n",
    "    # Average the loss over all mini-batches\n",
    "    loss_avg = sum(losses) / len(losses)\n",
    "\n",
    "    # Update the weights based on the accumulated gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Project the weights to a simplex\n",
    "    with torch.no_grad():\n",
    "        weights_unlabeled_new = project_simplex(weights_unlabeled)\n",
    "    \n",
    "    weights_unlabeled = weights_unlabeled_new.clone().detach().requires_grad_(True)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Average Sinkhorn loss: {loss_avg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Sinkhorn loss: 90.62239074707031\n",
      "Epoch 2, Sinkhorn loss: 157.9744415283203\n",
      "Epoch 3, Sinkhorn loss: 139.2010498046875\n",
      "Epoch 4, Sinkhorn loss: 139.2010498046875\n",
      "Epoch 5, Sinkhorn loss: 139.2010498046875\n",
      "Epoch 6, Sinkhorn loss: 139.2010498046875\n",
      "Epoch 7, Sinkhorn loss: 139.2010498046875\n",
      "Epoch 8, Sinkhorn loss: 139.2010498046875\n",
      "Epoch 9, Sinkhorn loss: 139.2010498046875\n",
      "Epoch 10, Sinkhorn loss: 139.2010498046875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from geomloss import SamplesLoss\n",
    "from torch import optim\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "#code to get a subset of mnist_trainset\n",
    "subset = torch.utils.data.Subset(mnist_trainset, range(990))\n",
    "unlabeled, target = torch.utils.data.random_split(subset, [900, 90])\n",
    "\n",
    "\n",
    "\n",
    "#unlabeled, target = torch.utils.data.random_split(mnist_trainset, [59744, 256])\n",
    "\n",
    "# Load the entire datasets\n",
    "unlabeled_images, _ = zip(*unlabeled)\n",
    "target_images, _ = zip(*target)\n",
    "\n",
    "unlabeled_images = torch.stack(unlabeled_images)\n",
    "target_images = torch.stack(target_images)\n",
    "\n",
    "# Create a loss function using GeomLoss\n",
    "sinkhorn_loss = SamplesLoss(loss=\"sinkhorn\", p=2, blur=0.05)\n",
    "\n",
    "# Initialize weights for the unlabeled_images\n",
    "weights_unlabeled = torch.full((len(unlabeled), 1), 1.0 / len(unlabeled), requires_grad=True)\n",
    "weights_target = torch.full((len(target), 1), 1.0 / len(target), requires_grad=False)\n",
    "# Define an optimizer\n",
    "optimizer = optim.SGD([weights_unlabeled], lr=0.01)\n",
    "\n",
    "# Define a function to project weights to a simplex\n",
    "def project_to_simplex(weights):\n",
    "    return torch.clamp(weights, min=0) / torch.sum(weights)\n",
    "\n",
    "def project_simplex(v):\n",
    "        \"\"\"\n",
    "        v: PyTorch Tensor to be projected to a simplex\n",
    "\n",
    "        Returns:\n",
    "        w: PyTorch Tensor simplex projection of v\n",
    "        \"\"\"\n",
    "        z = 1\n",
    "        orig_shape = v.shape\n",
    "        v = v.view(1, -1)\n",
    "        shape = v.shape\n",
    "        with torch.no_grad():\n",
    "            mu = torch.sort(v, dim=1)[0]\n",
    "            mu = torch.flip(mu, dims=(1,))\n",
    "            cum_sum = torch.cumsum(mu, dim=1)\n",
    "            j = torch.unsqueeze(torch.arange(1, shape[1] + 1, dtype=mu.dtype, device=mu.device), 0)\n",
    "            rho = torch.sum(mu * j - cum_sum + z > 0.0, dim=1, keepdim=True) - 1.\n",
    "            rho = rho.to(int)\n",
    "            max_nn = cum_sum[torch.arange(shape[0]), rho[:, 0]]\n",
    "            theta = (torch.unsqueeze(max_nn, -1) - z) / (rho.type(max_nn.dtype) + 1)\n",
    "            w = torch.clamp(v - theta, min=0.0).view(orig_shape)\n",
    "            return w\n",
    "\n",
    "# Loop over the datasets 10 times\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    # Compute Sinkhorn loss\n",
    "    loss = sinkhorn_loss(weights_unlabeled,unlabeled_images.view(unlabeled_images.shape[0], -1), weights_target,\n",
    "                         target_images.view(target_images.shape[0], -1)\n",
    "                        )\n",
    "\n",
    "    # Backpropagate the loss\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # Project the weights to a simplex\n",
    "    with torch.no_grad():\n",
    "        weights_unlabeled_new = project_simplex(weights_unlabeled)\n",
    "        \n",
    "    weights_unlabeled = weights_unlabeled_new.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Sinkhorn loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
